{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[reference script](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)"
      ],
      "metadata": {
        "id": "iTGn2Hjxs5E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Package and Data\n",
        "\n"
      ],
      "metadata": {
        "id": "38bH4kOQqgx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ojsMp4kVa2",
        "outputId": "1a98c218-d9fb-42f3-a700-8225a56b1898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tempfile\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the dataset\n"
      ],
      "metadata": {
        "id": "yAQLPt6DgLAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load your dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your CSV file\n",
        "# Make sure your CSV file contains features and a target column\n",
        "# For example, X as features and y as the target column\n",
        "# Modify this part according to your actual dataset structure\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Wildfire/Datasets/merged_tp_precip_wind_fmc_all.csv\")\n",
        "data.dropna(inplace = True)\n",
        "data.drop([\"Unnamed: 0\"], axis = 1, inplace = True)\n",
        "data.drop([\"START_DATE\"], axis = 1, inplace = True)\n",
        "data.drop([\"END_DATE\"], axis = 1, inplace = True)\n",
        "data.drop([\"DATE\"], axis = 1, inplace = True)\n",
        "\n",
        "#data[\"DATE\"] = data[\"DATE\"].astype(float)\n",
        "#data.drop([\"FIRE_SIZE\"], axis = 1, inplace = True)\n",
        "#data['FIRE_SIZE_CLASS'] = data['FIRE_SIZE_CLASS'].replace({'A':0 , 'B':1 , 'C':2 ,'D':3 , 'E':4 , 'F':5 , 'G':6 })\n"
      ],
      "metadata": {
        "id": "tNCZl80HArCa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(\"FIRE_SIZE_CLASS\").count()"
      ],
      "metadata": {
        "id": "ApYkX6G7EvYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-classification of the dataset with our own defined arces size"
      ],
      "metadata": {
        "id": "gpx-MHEygTC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def classify_ranges(df, column_name, ranges, classification_column):\n",
        "    for range_name, range_values in ranges.items():\n",
        "        df.loc[df[column_name].between(*range_values), classification_column] = range_name\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n",
        "classification_ranges = {\n",
        "    'A': (0, 99),\n",
        "    'B': (99.000000000001, 999),\n",
        "    'C': (999.000000000001, 99999999999999)\n",
        "}\n",
        "  # Creating a new column to store the classifications\n",
        "classify_ranges(data, 'FIRE_SIZE', classification_ranges, 'FIRE_SIZE_CLASS')\n",
        "data.drop([\"FIRE_SIZE\"], axis = 1, inplace = True)\n",
        "data.groupby(\"FIRE_SIZE_CLASS\").count()\n",
        "data['FIRE_SIZE_CLASS'] = data['FIRE_SIZE_CLASS'].replace({'A':0 , 'B':1 , 'C':2 ,'D':3 , 'E':4 , 'F':5 , 'G':6 })"
      ],
      "metadata": {
        "id": "V7MvyopPG1BN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your features and target column\n",
        "X = data.drop('FIRE_SIZE_CLASS', axis=1)  # Replace 'target_column_name' with your actual target column name\n",
        "y = data['FIRE_SIZE_CLASS']  # Replace 'target_column_name' with your actual target column name\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (optional but recommended)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define your neural network model for 7-class classification\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(7, activation='softmax')  # Use 'softmax' for 7-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_labels = y_pred.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDZTWzEq5DD1",
        "outputId": "9d8cf911-52e7-4372-cb38-3b84946991aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5458/5458 [==============================] - 11s 2ms/step - loss: 0.3490 - accuracy: 0.9048 - val_loss: 0.3330 - val_accuracy: 0.9060\n",
            "Epoch 2/10\n",
            "5458/5458 [==============================] - 8s 1ms/step - loss: 0.3285 - accuracy: 0.9058 - val_loss: 0.3239 - val_accuracy: 0.9063\n",
            "Epoch 3/10\n",
            "5458/5458 [==============================] - 9s 2ms/step - loss: 0.3229 - accuracy: 0.9060 - val_loss: 0.3195 - val_accuracy: 0.9063\n",
            "Epoch 4/10\n",
            "5458/5458 [==============================] - 8s 1ms/step - loss: 0.3187 - accuracy: 0.9059 - val_loss: 0.3175 - val_accuracy: 0.9065\n",
            "Epoch 5/10\n",
            "5458/5458 [==============================] - 8s 2ms/step - loss: 0.3159 - accuracy: 0.9060 - val_loss: 0.3150 - val_accuracy: 0.9068\n",
            "Epoch 6/10\n",
            "5458/5458 [==============================] - 8s 2ms/step - loss: 0.3134 - accuracy: 0.9061 - val_loss: 0.3147 - val_accuracy: 0.9060\n",
            "Epoch 7/10\n",
            "5458/5458 [==============================] - 8s 1ms/step - loss: 0.3115 - accuracy: 0.9059 - val_loss: 0.3129 - val_accuracy: 0.9062\n",
            "Epoch 8/10\n",
            "5458/5458 [==============================] - 9s 2ms/step - loss: 0.3098 - accuracy: 0.9060 - val_loss: 0.3102 - val_accuracy: 0.9066\n",
            "Epoch 9/10\n",
            "5458/5458 [==============================] - 9s 2ms/step - loss: 0.3083 - accuracy: 0.9062 - val_loss: 0.3095 - val_accuracy: 0.9064\n",
            "Epoch 10/10\n",
            "5458/5458 [==============================] - 8s 1ms/step - loss: 0.3069 - accuracy: 0.9060 - val_loss: 0.3068 - val_accuracy: 0.9065\n",
            "1365/1365 [==============================] - 1s 1ms/step - loss: 0.3068 - accuracy: 0.9065\n",
            "Test Loss: 0.3068, Test Accuracy: 0.9065\n",
            "1365/1365 [==============================] - 1s 870us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95     39563\n",
            "           1       0.55      0.03      0.06      3286\n",
            "           2       0.34      0.02      0.04       815\n",
            "\n",
            "    accuracy                           0.91     43664\n",
            "   macro avg       0.60      0.35      0.35     43664\n",
            "weighted avg       0.87      0.91      0.87     43664\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, y_pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrKSVWCtI5Gx",
        "outputId": "23733d00-f6ba-49b8-b28f-6e868631dbf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9063072554049102\n"
          ]
        }
      ]
    }
  ]
}