{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# When compare the date between data sets, compare the following day delta \n",
    "days_delta_range = [0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5, -6, 6, -7, 7, -8, 8, -9, 9, -10, 10, \n",
    "                    -11, 11, -12, 12, -13, 13, -14, 14, -15, 15, -16, 16, -17, 17, -18, 18, -19, 19, \n",
    "                    -21, 21, -22, 22, -23, 23, -24, 24, -25, 25, -26, 26, -27, 27, -28, 28, -29, 29, -20, 20,\n",
    "                    -31, 31, -32, 32, -33, 33, -34, 34, -35, 35, -36, 36, -37, 37, -38, 38, -39, 39, -30, 30\n",
    "                    ]\n",
    "# If a datasize size is less this is number, we will check its adjacent counties \n",
    "optimial_dataset_size = 10\n",
    "\n",
    "# Extract the whole volume \n",
    "def filter_dataframe_by_value(df, column_name, value_to_find):\n",
    "    filtered_rows = df[df[column_name] == value_to_find]\n",
    "    return filtered_rows\n",
    "\n",
    "def load_csv(target):\n",
    "    # Load the target dataset\n",
    "    target_dataset = pd.read_csv(target['file_name']) #, index_col=target['date_column_name'])\n",
    "    print(f\"The file {target['file_name']} contains data {target_dataset.shape}\")\n",
    "    return target_dataset\n",
    "\n",
    "def save_csv(target, target_dataset):  \n",
    "    # Drop all the index columns unnamed: 0 before saving \n",
    "    target_dataset.drop(target_dataset.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "    # Save the data to its original file\n",
    "    target_dataset.to_csv(target['file_name'])\n",
    "    print(f\"Saved to {target['file_name']}\")\n",
    "\n",
    "# Copy the matched column from the referral dataset to the target dataset\n",
    "# The match key is the date. Note the date column might have different names\n",
    "# The datetime must be close to each other between two datasets\n",
    "def merge_dataframes_on_near_date(target_df, target, \n",
    "                                  referral_df, referral):\n",
    "\n",
    "    #referral_df.head()\n",
    "    \n",
    "    # Sort the referral data set by Date, ascending \n",
    "    referral_df = referral_df.sort_values(referral['date_column_name'])\n",
    "\n",
    "    for index1, row1 in target_df.iterrows():\n",
    "        # Get the target datetime. \n",
    "        # Need to convert the string to a datetime object\n",
    "        row1_datetime = datetime.strptime(str(row1[target['date_column_name']]),  target['datetime_format'])\n",
    "\n",
    "        row1_near_date = []\n",
    "        for i in days_delta_range:\n",
    "\n",
    "            # Convert the day to int in order to compare with the referral_df.date, which is loaded as int by default\n",
    "            next_day = row1_datetime + timedelta( days = i)\n",
    "            #referral_next_day = int(next_day.strftime(referral['datetime_format']))\n",
    "            referral_next_day = next_day.strftime(referral['datetime_format'])\n",
    "            # If it is int ( not a string), convert to int as it is fast\n",
    "            if  referral_df.dtypes[referral['date_column_name']] == 'int':\n",
    "                referral_next_day = int(referral_next_day)\n",
    "            row1_near_date.append( referral_next_day)\n",
    "\n",
    "            # Find in the referral dataframe by date\n",
    "            found_df = referral_df[referral_df[referral['date_column_name']] == referral_next_day]\n",
    "            if (found_df.size > 0):\n",
    "                print(f\"Add data to the original {row1_datetime} <- {referral_next_day}  delta days {i}: \")\n",
    "                for f in referral[\"data_fields_to_be_copied\"]:\n",
    "                    # Found the value in the first cell and assign to the target dataframe\n",
    "                    # The target columne name might be different from the referral's. \n",
    "                    ### Hard code for now\n",
    "                    target_column_name = f;\n",
    "                    if target_column_name == \"percent\":\n",
    "                        target_column_name = \"fmc\"\n",
    "\n",
    "                    target_df.at[index1, target_column_name] = found_df.iloc[0][f]\n",
    "                    print(f\"{f} = {target_df.at[index1, target_column_name]}\")\n",
    "                \n",
    "                # Finished fo this date\n",
    "                break;\n",
    "\n",
    "#\n",
    "# Check the adjacent county if the original county has not data in a referral dataset\n",
    "#\n",
    "def get_datasize_by_fips(referral_dataset, referral, county_fips_code):\n",
    "    referral_count = referral_dataset[referral_dataset[referral['fips_column_name']] == county_fips_code].shape[0]\n",
    "    print(f\"The county {county_fips_code} has {referral_count} rows of data in the {referral['file_name']}\")  \n",
    "    return referral_count\n",
    "\n",
    "#\n",
    "# Check all adjacent counties to find the max value\n",
    "# The adj county with the max value will replace the referral[county] data\n",
    "# Return the max value \n",
    "#\n",
    "def count_adjacent_county(referral_dataset, referral):\n",
    "    \n",
    "    adjacent_county_df = pd.read_csv(\"Datasets/county_adjacency.csv\")\n",
    "    rows = None\n",
    "    rows = adjacent_county_df[adjacent_county_df['fips'] == referral['fips_code']]\n",
    "    i = 1\n",
    "    max_count = 0\n",
    "    max_county_name = \"\"\n",
    "    max_county_fips = 0\n",
    "    while ( rows is not None):\n",
    "        index = rows.first_valid_index()\n",
    "        if ( index is None):\n",
    "            break;\n",
    "        next_row = None\n",
    "        try:\n",
    "            next_row = adjacent_county_df.iloc[index + i]\n",
    "        except Exception as e:\n",
    "            # We might have reached to the end of the file\n",
    "            break\n",
    "        i += 1\n",
    "        if ( next_row is not None):\n",
    "            # check if the row has a county name, which means the end of ajacent county list\n",
    "            if ( pd.isna(next_row['county']) == False):\n",
    "                break\n",
    "            \n",
    "            adjacent_county = next_row['adjacent_county'].split(', ')[0]\n",
    "            adjacent_county_fips = next_row[3] #'adjacent_county_fips'\n",
    "            referral_count = get_datasize_by_fips(referral_dataset, referral, adjacent_county_fips)\n",
    "            if (referral_count > max_count):\n",
    "                max_count = referral_count\n",
    "                max_county_name = adjacent_county\n",
    "                max_county_fips = adjacent_county_fips\n",
    "    # End of While\n",
    "\n",
    "    # If we have an adj county with max size\n",
    "    if ( max_count > 0):\n",
    "        print(f\"*** Using the adjacent county {max_county_name}, {max_county_fips} of {referral['referral_county_name']}. It has {max_count} rows of data in the {referral['file_name']}\")  \n",
    "        referral['referral_county_name'] = max_county_name\n",
    "        referral['fips_code'] = max_county_fips\n",
    "    # return 0 if not adjcent counties have any data\n",
    "    # print(\"The county doesn't exist in the referral dataset. Exits the program.\")\n",
    "    return max_count\n",
    "\n",
    "#\n",
    "# Merge data from a referral dataset to a target \n",
    "# Return the number of rows merged\n",
    "#\n",
    "def merge_data_referral_to_target(target, referral): \n",
    "\n",
    "    # Load the target dataset\n",
    "    target_dataset = load_csv(target)\n",
    "\n",
    "    # Initial the referral dataset from a csv\n",
    "    referral_dataset = load_csv(referral)\n",
    "\n",
    "    # Check if the county exists in the referral dataset\n",
    "    count = get_datasize_by_fips(referral_dataset, referral, referral['fips_code'])\n",
    "    # If enough data is found, don't chck its adjacent counties \n",
    "    if ( count < optimial_dataset_size):\n",
    "        # Check adjacent county if not enough data found in the original county\n",
    "        count = count_adjacent_county( referral_dataset, referral)\n",
    "        # If no data is found, exit the program\n",
    "        if ( int(count) <= 0):\n",
    "            print(\"!!! ### !!! Neither the given county nor its adjcent counties exist in the referral dataset. Exits the program.\")\n",
    "            return 0\n",
    "    \n",
    "    # Create a new data frame containing only rows with the specific value\n",
    "    referral_dataset_fips_only = filter_dataframe_by_value(referral_dataset, referral['fips_column_name'], referral['fips_code'])\n",
    "    print(f\"referral_dataset with fips {referral['fips_code']} = {referral_dataset_fips_only.shape}\")\n",
    "                \n",
    "    # Add the new data from the referral dataset to the target dataset\n",
    "    # merge_dataframes_on_match\n",
    "    merge_dataframes_on_near_date(target_dataset, target, \n",
    "                                  referral_dataset_fips_only, referral)\n",
    "\n",
    "    # Save the data to its original file\n",
    "    save_csv(target, target_dataset)\n",
    "\n",
    "    return target_dataset.shape[0]\n",
    "\n",
    "def generate_csv_for_fips(target):\n",
    "\n",
    "    fire_data = pd.read_csv(\"Datasets/aggregated_wildfire.csv\")\n",
    "    # Create a new DataFrame containing only rows with the specific value\n",
    "    firedata_fips = filter_dataframe_by_value(fire_data, 'FIPS', target[\"fips\"])\n",
    "    #new columns for merged features\n",
    "    firedata_fips[\"tmax\"] = pd.NA\n",
    "    firedata_fips[\"tmin\"] = pd.NA\n",
    "    firedata_fips[\"tavg\"] = pd.NA\n",
    "    firedata_fips[\"prcp\"] = pd.NA\n",
    "    firedata_fips[\"aws\"] = pd.NA\n",
    "    firedata_fips[\"fmc\"] = pd.NA\n",
    "    save_csv(target, firedata_fips)\n",
    "\n",
    "# Drop rows with nan columns\n",
    "def drop_ana(target):\n",
    "    df = load_csv(target)\n",
    "    df = df.dropna() #(axis=0, how='any')\n",
    "    save_csv(target, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
